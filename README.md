## Pre-trained word and phrase vectors
### Others
- [agiga.bin](https://code.google.com/archive/p/word2vec/) : trained on [Annotated English Gigaword](https://catalog.ldc.upenn.edu/LDC2012T21). dim = 300


### [Word2Vec Homepage](https://code.google.com/archive/p/word2vec/)
- [GoogleNews-vectors-negative300.bin](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) : trained on part of Google News dataset (about 100 billion words). dim = 300
- [freebase-vectors-skipgram1000.bin](https://docs.google.com/file/d/0B7XkCwpI5KDYaDBDQm1tZGNDRHc/edit?usp=sharing) : Entity vectors trained on 100B words from various news articles.
- [freebase-vectors-skipgram1000-en.bin](https://docs.google.com/file/d/0B7XkCwpI5KDYeFdmcVltWkhtbmM/edit?usp=sharing) : Entity vectors trained on 100B words from various news articles, using the deprecated /en/ naming (more easily readable); the vectors are sorted by frequency. 
